
import io
import base64
import google.generativeai as genai
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from statsmodels.tsa.stattools import adfuller, grangercausalitytests
from sklearn.preprocessing import StandardScaler
from statsmodels.tools.eval_measures import aic, bic
import statsmodels.api as sm
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit
from sklearn.linear_model import Lasso, LinearRegression
from sklearn.metrics import mean_squared_error
import streamlit as st
import os


# ä¸Šä¼ çš„æ–‡ä»¶ä¼šä¿å­˜åœ¨æ‰§è¡Œä¸‹é¢è¯­å¥çš„ç›®å½•
# streamlit run ./streamlit_version.py


class GrangerCausalityAnalyzer:
    def __init__(self, data, maxlag=2, adf_significance=0.05, causality_significance=0.05):
        """
        Initializes the Granger Causality Analyzer.

        Parameters:
        - data: DataFrame containing the time-series variables.
        - maxlag: Maximum number of lags for Granger Causality tests (default=4).
        - adf_significance: Significance level for ADF test (default=0.05).
        - causality_significance: Significance level for Granger Causality test (default=0.05).
        """
        self.data = data
        self.maxlag = maxlag
        self.adf_significance = adf_significance
        self.causality_significance = causality_significance
        self.stationary_vars = {}
        self.stationarity_summary = {}
        self.causality_results = []

    def adf_test(self, series, name):
        """Performs the Augmented Dickey-Fuller (ADF) test for stationarity."""
        result = adfuller(series.dropna(), autolag='AIC')
        print(f'ADF Test for {name}:')
        print(f'ADF Statistic: {result[0]}')
        print(f'p-value: {result[1]}')
        print(f'Critical Values: {result[4]}')

        if result[1] <= self.adf_significance:
            print(f'{name} is stationary (reject null hypothesis)\n')
            return True, result[1]
        else:
            print(f'{name} is non-stationary (fail to reject null hypothesis)\n')
            return False, result[1]

    def transform_to_stationary(self):
        """Transforms non-stationary variables to stationary using differencing."""
        variables = [col for col in self.data.columns if col != 'Date']
        data_transformed = self.data[variables].copy()

        for var in variables:
            is_stationary, p_value = self.adf_test(data_transformed[var], var)
            if is_stationary:
                self.stationary_vars[var] = var
                self.stationarity_summary[var] = f"stationary (p-value = {p_value:.4f})"
            else:
                # First difference
                data_transformed[f'{var}_diff'] = data_transformed[var].diff()
                is_stationary, p_value = self.adf_test(data_transformed[f'{var}_diff'], f'Differenced {var}')
                if is_stationary:
                    self.stationary_vars[var] = f'{var}_diff'
                    self.stationarity_summary[var] = f"non-stationary, stationary after first differencing (p-value = {p_value:.4f})"
                else:
                    # Second difference
                    data_transformed[f'{var}_diff2'] = data_transformed[f'{var}_diff'].diff()
                    is_stationary, p_value = self.adf_test(data_transformed[f'{var}_diff2'], f'Second Differenced {var}')
                    if is_stationary:
                        self.stationary_vars[var] = f'{var}_diff2'
                        self.stationarity_summary[var] = f"non-stationary, stationary after second differencing (p-value = {p_value:.4f})"
                    else:
                        self.stationary_vars[var] = f'{var}_diff2'
                        self.stationarity_summary[var] = f"non-stationary even after second differencing (p-value = {p_value:.4f})"

        # Create final stationary dataset
        stationary_columns = [self.stationary_vars[var] for var in variables]
        data_stationary = data_transformed[stationary_columns].dropna()
        data_stationary.columns = variables  # Rename columns to original names for clarity
        self.data_stationary = data_stationary

        print("Stationary dataset created:")
        print(self.data_stationary.head())
        print("\nNaN values in stationary dataset:")
        print(self.data_stationary.isna().sum())

        return self.data_stationary

    def run_granger_causality_tests(self):
        """Runs Granger Causality tests on stationary variables."""
        print("\nRunning Granger Causality Tests...\n")
        variables = list(self.stationary_vars.keys())

        for i, var1 in enumerate(variables):
            for j, var2 in enumerate(variables):
                if i != j:
                    print(f'Granger Causality Test: {var2} -> {var1}')
                    test_results = grangercausalitytests(self.data_stationary[[var1, var2]], maxlag=self.maxlag, verbose=True)

                    # Parse the results dictionary
                    for lag, result in test_results.items():
                        p_value = result[0]['ssr_ftest'][1]  # Extract p-value
                        if p_value <= self.causality_significance:
                            self.causality_results.append({'cause': var2, 'effect': var1, 'lag': lag, 'p_value': p_value})

                    print("\n" + "="*50 + "\n")

    def generate_conclusion(self):
        """Generates a summary of the findings from the Granger Causality tests."""
        print("\nConclusion: Granger Causality Analysis of Economic Indicators\n")

        # Stationarity Summary
        print("Stationarity Analysis:")
        for var, summary in self.stationarity_summary.items():
            print(f"- {var}: {summary}")
        print("\n")

        # Granger Causality Findings
        print("Granger Causality Findings:")
        if self.causality_results:
            for result in self.causality_results:
                print(f"- {result['cause']} Granger-causes {result['effect']} at lag {result['lag']} (p-value = {result['p_value']:.4f})")
        else:
            print("- No significant Granger Causality relationships were found.")
        print("\n")

    def run_workflow(self):
        """Executes the full workflow: transformation, Granger Causality, and conclusion."""
        self.transform_to_stationary()
        self.run_granger_causality_tests()
        self.generate_conclusion()


class TimeSeriesRegression:
    def __init__(self, df, features, target, max_lag=6):
        """
        Initializes the TimeSeriesRegression model.

        Parameters:
            df (pd.DataFrame): The dataset.
            features (list): List of feature column names.
            target (str): Target variable.
            max_lag (int): Maximum lag to consider.
        """
        self.df = df.copy()
        self.features = features
        self.target = target
        self.max_lag = max_lag
        self.best_lags = {}

    def find_best_lags(self):
        """Finds the optimal lag for each feature using time-series cross-validation."""
        tscv = TimeSeriesSplit(n_splits=5)

        for feature in self.features:
            best_mse = float('inf')
            best_lag = 0

            for lag in range(1, self.max_lag + 1):
                df_lagged = self.df[[feature, self.target]].copy()
                df_lagged[f"{feature}_lag"] = df_lagged[feature].shift(lag)
                df_lagged.dropna(inplace=True)

                X = df_lagged[[f"{feature}_lag"]]
                y = df_lagged[self.target]

                mse_list = []

                for train_index, test_index in tscv.split(X):
                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

                    model = LinearRegression()
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)

                    mse = mean_squared_error(y_test, y_pred)
                    mse_list.append(mse)

                avg_mse = np.mean(mse_list)

                if avg_mse < best_mse:
                    best_mse = avg_mse
                    best_lag = lag

            self.best_lags[feature] = best_lag  # Store best lag

    def create_lagged_dataset(self):
        """Creates a DataFrame with the best lagged features."""
        self.df_lagged = self.df.copy()
        for feature, lag in self.best_lags.items():
            self.df_lagged[f"{feature}_lag"] = self.df_lagged[feature].shift(lag)

        self.df_lagged.dropna(inplace=True)  # Remove NaN rows
        return self.df_lagged

    def train_model(self):
        """Trains a multiple linear regression model using the best lags."""
        X = self.df_lagged[[f"{feat}_lag" for feat in self.best_lags.keys()]]
        y = self.df_lagged[self.target]

        self.model = LinearRegression()
        self.model.fit(X, y)

        # Predict & Evaluate
        self.y_pred = self.model.predict(X)
        self.mse = mean_squared_error(y, self.y_pred)
        print(f"Final Model MSE: {self.mse}")

    def plot_actual_vs_predicted(self):
        """Plot actual vs predicted values."""
        plt.figure(figsize=(10, 6))
        plt.scatter(self.df_lagged[self.target], self.y_pred, color='blue', alpha=0.6, label="Actual vs Predicted")
        plt.plot(self.df_lagged[self.target], self.df_lagged[self.target], color='red', linestyle='--', label="Perfect Fit")
        plt.xlabel("Actual " + self.target)
        plt.ylabel("Predicted " + self.target)
        plt.title(f"Actual vs Predicted: {self.target}")
        plt.legend()
        plt.grid(True)
        plt.show()

    def plot_residuals(self):
        """Plot residuals of the model."""
        residuals = self.df_lagged[self.target] - self.y_pred
        plt.figure(figsize=(10, 6))
        plt.scatter(self.df_lagged[self.target], residuals, color='green', alpha=0.6, label="Residuals")
        plt.axhline(y=0, color='red', linestyle='--', label="Zero Residual Line")
        plt.xlabel("Actual " + self.target)
        plt.ylabel("Residuals")
        plt.title(f"Residuals Plot: {self.target}")
        plt.legend()
        plt.grid(True)
        plt.show()

    def run(self):
        """Executes the full pipeline: find best lags, create dataset, train model, and plot results."""
        print("Finding best lags...")
        self.find_best_lags()
        print(f"Best lags: {self.best_lags}")

        print("Creating lagged dataset...")
        self.create_lagged_dataset()

        print("Training model...")
        self.train_model()

        # # Plotting results
        # self.plot_actual_vs_predicted()
        # self.plot_residuals()
        # åˆ†åˆ«ç»˜åˆ¶å¹¶æ˜¾ç¤ºä¸¤å¼ å›¾
        # import streamlit as st
        st.subheader("Actual vs Predicted")
        self.plot_actual_vs_predicted()  # ç»˜åˆ¶å®é™…å€¼ vs é¢„æµ‹å€¼
        st.pyplot(plt.gcf())  # æ˜¾ç¤ºå½“å‰å›¾å½¢
        plt.clf()  # æ¸…é™¤å½“å‰å›¾å½¢ï¼Œé¿å…å¹²æ‰°ä¸‹ä¸€å¼ å›¾

        st.subheader("Residuals")
        self.plot_residuals()  # ç»˜åˆ¶æ®‹å·®å›¾
        st.pyplot(plt.gcf())  # æ˜¾ç¤ºå½“å‰å›¾å½¢
        plt.clf()  # æ¸…é™¤å›¾å½¢
#----------------------------------------------------------------------------------------------------------------------
def upload():
    st.title("Upload data")
    # å¤šæ–‡ä»¶ä¸Šä¼ 
    uploaded_files = st.file_uploader("Please select one or more files to upload", type=["csv"], accept_multiple_files=True)
    # ä¿å­˜æ¯ä¸ªæ–‡ä»¶
    if uploaded_files:
        save_dir = "./data"
        os.makedirs(save_dir, exist_ok=True)  # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        for uploaded_file in uploaded_files:
            filename = uploaded_file.name
            save_path = os.path.join(save_dir, filename)
            # å†™å…¥æ–‡ä»¶
            with open(save_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
        st.success(f"âœ… Saved {len(uploaded_files)} file")

def detect_date_column(df: pd.DataFrame) -> str:
    """
    Automatically detects the column in the DataFrame that contains date-like information,
    including quarterly data in the format "YYYY Qx".

    Parameters:
        df (pd.DataFrame): The DataFrame to search for date columns.

    Returns:
        str: The name of the detected date column, or None if no date column is found.
    """
    for column in df.columns:
        try:
            # Convert column to string if it's not already a string type
            if df[column].dtype != 'O':  # Not object type (string)
                df[column] = df[column].astype(str)

            # Check if column values are in "YYYY Qx" format (e.g., "2021 Q1")
            if df[column].str.contains(r'\d{4} Q[1-4]', na=False).any():
                return column  # Return the column name if it contains "YYYY Qx" format

            # Try to convert the column to datetime for standard date formats
            pd.to_datetime(df[column], errors='raise')
            return column  # Return the column name if it's a valid date column
        except (ValueError, TypeError):
            continue
    return None  # Return None if no valid date column is found

def convert_quarter_to_date(quarter_str: str) -> pd.Timestamp:
    """
    Converts a quarterly string (e.g., "2021 Q1") to the corresponding date (e.g., "2021-01-01").
    This function is flexible and can work with any quarterly string containing "YYYY Qx".

    Parameters:
        quarter_str (str): The quarterly string (e.g., "2021 Q1").

    Returns:
        pd.Timestamp: The corresponding datetime object (e.g., "2021-01-01").
    """
    # Split the string into year and quarter parts
    parts = quarter_str.split(" Q")
    if len(parts) == 2:
        year, quarter = parts
        # Handle the case where the string contains a valid year and quarter
        try:
            year = int(year)
            quarter = int(quarter)
            # Map quarters to the first month of each quarter
            quarter_months = {1: 1, 2: 4, 3: 7, 4: 10}
            if quarter in quarter_months:
                month = quarter_months[quarter]
                return pd.to_datetime(f"{year}-{month:02d}-01")
        except ValueError:
            pass  # If parsing fails, return NaT (could be invalid data)
    return pd.NaT  # Return NaT if the input string is not in a valid "YYYY Qx" format

def clean_dataset(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleans a DataFrame by stripping column names and converting date columns to datetime format.
    If a column is detected as a date, it is renamed to 'Date'.

    Parameters:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        pd.DataFrame: The cleaned DataFrame.
    """
    # Clean column names (remove leading/trailing spaces)
    df.columns = df.columns.str.strip()

    date_column = detect_date_column(df)

    if date_column:
        # Check if the column contains quarterly-like data (e.g., "YYYY Qx")
        if df[date_column].dtype == object and df[date_column].str.contains(r'\d{4} Q[1-4]', na=False).any():
            # Convert the "YYYY Qx" format to datetime dynamically
            df[date_column] = df[date_column].apply(convert_quarter_to_date)
        else:
            # For standard date formats, convert to datetime
            df[date_column] = pd.to_datetime(df[date_column], errors="coerce")

        # Rename the date column to 'Date'
        df.rename(columns={date_column: 'Date'}, inplace=True)

    return df

def load_dataset(folder_path: str = "data", crisis_years: list = []) -> dict:
    crisis_data = {year: [] for year in crisis_years}

    for file_name in os.listdir(folder_path):
        if file_name.endswith(".csv"):
            file_path = os.path.join(folder_path, file_name)
            df = pd.read_csv(file_path)

            df = clean_dataset(df)
            date_column = detect_date_column(df)

            if date_column:
                df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
                df["Year"] = df[date_column].dt.year

                unique_years = df["Year"].unique()
                for year in crisis_years:
                    if year in unique_years:
                        crisis_data[year].append((file_name, df))  # ä¿å­˜æ–‡ä»¶åå’Œ DataFrame

                df.drop(columns=["Year"], inplace=True)

    return crisis_data

def normalisation(df, normalization_type='z-score', columns=None):
    """
    Normalizes the numeric columns of a DataFrame using the specified normalization method.

    Args:
    - df (pd.DataFrame): The DataFrame to normalize.
    - normalization_type (str): The type of normalization to apply. Options are 'z-score' or 'min-max'. Default is 'z-score'.
    - columns (list or None): List of column names to normalize. If None, normalizes all numeric columns.

    Returns:
    - pd.DataFrame: DataFrame with normalized values.
    """
    # Select numeric columns
    numeric_columns = df.select_dtypes(include=['number']).columns.tolist()

    if columns is not None:
        # Use provided columns, ensuring they are numeric
        numeric_columns = [col for col in columns if col in numeric_columns]

    if len(numeric_columns) == 0:
        raise ValueError("No numeric columns to normalize in the provided DataFrame.")

    # Select the normalization method
    if normalization_type == 'z-score':
        scaler = StandardScaler()
    elif normalization_type == 'min-max':
        scaler = MinMaxScaler()
    else:
        raise ValueError(f"Unknown normalization type: {normalization_type}. Supported types are 'z-score' and 'min-max'.")

    # Apply the scaler to the numeric columns
    normalized_data = scaler.fit_transform(df[numeric_columns])

    # Convert the normalized data back to a DataFrame
    normalized_df = df.copy()
    normalized_df[numeric_columns] = normalized_data

    return normalized_df

def adjust_column_for_periods(df: pd.DataFrame, periods: list):
    """
    Adjusts specified columns in a DataFrame based on multiple periods with varying start/end dates and values.

    Args:
    - df (pd.DataFrame): The DataFrame containing the data.
    - periods (list): A list of dictionaries, where each dictionary contains:
        - 'start': start date of the period (str or datetime)
        - 'end': end date of the period (str or datetime)
        - 'column': the name of the column to adjust (str)
        - 'value': the adjustment value to apply during the period (int, float, or str)

    Returns:
    - pd.DataFrame: The adjusted DataFrame.
    """
    # Copy the original DataFrame to avoid modifying the input DataFrame directly
    adjusted_df = df.copy()

    for period in periods:
        # Convert start and end dates to datetime
        start_date = pd.to_datetime(period['start'])
        end_date = pd.to_datetime(period['end'])

        # Check if the column exists in the DataFrame
        column = period['column']
        if column not in adjusted_df.columns:
            raise ValueError(f"Column '{column}' not found in the DataFrame.")

        # Apply the adjustment based on the period
        adjusted_df.loc[
            (adjusted_df['Date'] >= start_date) &
            (adjusted_df['Date'] <= end_date),
            column
        ] = period['value']

    return adjusted_df

def preprocess_data(df: pd.DataFrame, freq: str = 'MS', method: str = 'ffill') -> pd.DataFrame:
    """
    Preprocesses a DataFrame by converting date formats and resampling it to a given frequency.

    This version supports various types of date formats and resampling methods.

    Parameters:
        df (pd.DataFrame): The DataFrame containing the data.
        freq (str): The resampling frequency. Default is 'MS' (monthly start).
                    Other options include 'D' (daily), 'M' (monthly), 'Y' (yearly), etc.
        method (str): Method for filling missing data during resampling. Default is 'ffill' (forward fill).
                      Other options: 'bfill' (backward fill), 'pad' (pad), 'nearest', etc.

    Returns:
        pd.DataFrame: Preprocessed DataFrame with resampled data.
    """
    df = df.dropna(subset=['Date'])

    df_resampled = df.set_index('Date').resample(freq).apply(method).reset_index()

    return df_resampled

def merge_dataframes_on_date(*dfs):
    """
    Merges multiple dataframes on the 'Date' column.

    Parameters:
        *dfs: Multiple DataFrames to merge.

    Returns:
        pd.DataFrame: A single merged DataFrame on the 'Date' column.
    """
    # Preprocess each DataFrame before merging
    processed_dfs = [preprocess_data(df.copy()) for df in dfs]

    # Merge DataFrames on 'Date'
    merged_df = processed_dfs[0]
    for df in processed_dfs[1:]:
        merged_df = pd.merge(merged_df, df, on='Date', how='inner')

    return merged_df

def filter_target_column(all_dfs, crisis_year, target_column):
    """
    Filters each DataFrame in the all_dfs list for a given 'target_column' and 'Date' column.

    Parameters:
        all_dfs (list): List of DataFrames to filter.
        crisis_year (str): The year or key indicating which group of DataFrames to filter.
        target_column (str): The column to filter along with 'Date'.

    Returns:
        None: Modifies the DataFrames in place.
    """
    # Loop through each DataFrame in the selected crisis_year
    for i, df in enumerate(all_dfs[crisis_year]):
        if target_column in df.columns:
            # Filter the DataFrame to only include 'Date' and the target column
            df_filtered = df[['Date', target_column]].copy()

            # Optionally, assign it back to the list to modify in place
            all_dfs[crisis_year][i] = df_filtered

def plotting(
        df: pd.DataFrame,
        file_name: str = " ",
        step_column: str = " ",
        x_label: str = None,
        y_label: str = None,
        annotation: dict = None,
        highlight: list = None
):
    """
    é€‚é…Streamlitçš„ç»˜å›¾å‡½æ•°ï¼Œæ”¯æŒäº‹ä»¶æ ‡æ³¨å’Œé«˜äº®å‘¨æœŸ

    Parameters:
        df (pd.DataFrame): è¾“å…¥æ•°æ®
        file_name (str): æ•°æ®é›†æ–‡ä»¶åï¼ˆç”¨äºæ ‡é¢˜ï¼‰
        step_column (str): ç»˜åˆ¶ä¸ºé˜¶æ¢¯å›¾çš„åˆ—å
        x_label (str): Xè½´æ ‡ç­¾
        y_label (str): Yè½´æ ‡ç­¾
        annotation (dict): äº‹ä»¶æ ‡æ³¨ï¼Œæ ¼å¼ä¸º {"Event Name": ("YYYY-MM-DD", with_line)}
        highlight (list): é«˜äº®å‘¨æœŸï¼Œæ ¼å¼ä¸º [(start_date, end_date, price_level, label, color), ...]

    Returns:
        matplotlib.figure.Figure: ç»˜åˆ¶çš„å›¾å½¢å¯¹è±¡
    """
    # è‡ªåŠ¨æ£€æµ‹æ—¥æœŸåˆ—
    date_col = None
    for col in df.columns:
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            date_col = col
            break

    if date_col is None:
        raise ValueError("æœªæ£€æµ‹åˆ°æ—¥æœŸåˆ—ï¼Œè¯·ç¡®ä¿åŒ…å«datetimeç±»å‹åˆ—")

    # è®¾ç½®é»˜è®¤æ ‡é¢˜
    title = f"{file_name.removesuffix('.csv')} - Time Series Analysis"

    # åˆ›å»ºå›¾å½¢å¯¹è±¡
    fig, ax1 = plt.subplots(figsize=(15, 8))
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

    # ç»˜åˆ¶ä¸»æ•°æ®
    for i, column in enumerate(df.columns[df.columns != date_col]):
        line_style = '-'
        marker_style = 'o'

        if step_column == column:
            ax1.step(df[date_col], df[step_column], where='post',
                     color=colors[i % len(colors)], linewidth=2,
                     label=f'{step_column} (Step)')
        else:
            ax1.plot(df[date_col], df[column], linestyle=line_style, marker=marker_style,
                     linewidth=2, markersize=6, color=colors[i % len(colors)], label=column)

    # ç»˜åˆ¶é«˜äº®å‘¨æœŸï¼ˆæŸ±çŠ¶å›¾ï¼‰
    if highlight:
        for start, end, price, label, color in highlight:
            width = (end - start).days
            middle = start + (end - start) / 2
            ax1.bar(x=start, height=price, width=width, color=color, alpha=0.2, align='edge', label=label)
            # æ·»åŠ æ ‡ç­¾
            ax1.annotate(label, xy=(middle, price), xytext=(middle, price + 200), arrowprops=dict(arrowstyle='->', color=color), color=color, fontsize=12, ha='center')

    # ç»˜åˆ¶äº‹ä»¶æ ‡æ³¨
    if annotation:
        y_max = df.select_dtypes(include=['number']).max().max()  # è·å–æ•°æ®æœ€å¤§å€¼
        for event_name, (event_date, with_line) in annotation.items():
            event_date = pd.to_datetime(event_date)
            # ç»˜åˆ¶å‚ç›´çº¿ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if with_line:
                ax1.axvline(x=event_date, color='red', linestyle='-', linewidth=2, label=event_name)
            # æ·»åŠ äº‹ä»¶æ ‡æ³¨
            y_position = y_max - (200 if with_line else 100)
            ax1.annotate(event_name, xy=(event_date, y_position), xytext=(event_date + pd.DateOffset(days=150), y_position + 200), fontsize=12, color='red', ha='center', arrowprops=dict(arrowstyle='->', color='blue' if not with_line else 'red'))

    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax1.set_xlabel(x_label or date_col)
    ax1.set_ylabel(y_label or "Value")
    ax1.set_title(title)
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()

    return fig

def select_data():
    # æ·»åŠ å¹´ä»½è¾“å…¥æ¡†
    st.title("Load data based on year")
    # å¤šé€‰å¹´ä»½
    available_years = list(range(1970, 2026))  # ä½ ä¹Ÿå¯ä»¥æ¢æˆä½ æƒ³è¦å±•ç¤ºçš„å¹´ä»½èŒƒå›´
    # è®¾ç½®é»˜è®¤å¹´ä»½
    default_years = [2008, 2022]

    # å…è®¸ç”¨æˆ·é€‰æ‹©å¤šä¸ªå¹´ä»½ï¼Œé»˜è®¤é€‰ä¸­ 2008 å’Œ 2022
    crisis_years = st.multiselect("Select one or more crisis years",options=available_years,default=default_years)

    # å¦‚æœæ²¡æœ‰é€‰æ‹©å¹´ä»½ï¼Œæ˜¾ç¤ºé»˜è®¤å€¼
    if not crisis_years:
        st.warning("Please select at least one year. Defaulting to 2008 and 2022.")
        crisis_years = default_years  # ä½¿ç”¨é»˜è®¤å¹´ä»½

    # æ’åºé€‰ä¸­çš„å¹´ä»½åˆ—è¡¨
    crisis_years = sorted(crisis_years)
    # å°†åˆšåˆšçš„æ‰§è¡Œä¿¡æ¯ä¿å­˜åœ¨sessionä¸­
    if st.button("Load Datasets"):
        st.session_state["datasets_loaded"] = True
        st.session_state["crisis_years"] = crisis_years

    if st.session_state.get("datasets_loaded"):
        crisis_years = st.session_state["crisis_years"]
        with st.spinner("Loading datasets..."):
            all_dfs = load_dataset("./data", crisis_years)
            # æ˜¾ç¤ºåŠ è½½æˆåŠŸçš„åé¦ˆ
            if all_dfs:
                st.success(f"Datasets loaded successfully for years: {', '.join(map(str, crisis_years))}")

                # æ˜¾ç¤ºæ¯ä¸ªå¹´ä»½åŠ è½½çš„æ–‡ä»¶æ•°é‡ï¼ˆæŒ‰å¹´ä»½æŠ˜å ï¼‰
                for year, file_df_list in all_dfs.items():
                    if file_df_list:
                        # æ¯ä¸ªå¹´ä»½ä¸€ä¸ªæŠ˜å é¢æ¿
                        with st.expander(f"ğŸ“… Year {year} ({len(file_df_list)} datasets)", expanded=False):
                            # æ•°æ®é›†åˆ—è¡¨ï¼ˆä½¿ç”¨ç­‰å®½å­—ä½“æ›´æ•´é½ï¼‰
                            for file_name, _ in file_df_list:
                                st.code(file_name, language="text")

                            # æ·»åŠ ä¸€ä¸ªå¿«é€Ÿæ“ä½œæŒ‰é’®ï¼ˆå¯é€‰ï¼‰
                            col1, col2 = st.columns(2)
                            with col1:
                                if st.button(f"Analyze all {year} data", key=f"analyze_{year}"):
                                    st.session_state['selected_year'] = year
                            with col2:
                                if st.button(f"Download file list", key=f"download_{year}"):
                                    # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸‹è½½é€»è¾‘
                                    st.toast(f"File list for {year} prepared")
                # æ˜¾ç¤ºæ€»æ•°é‡
                total_dfs = sum(len(df_list) for df_list in all_dfs.values())
                st.write(f"**Total datasets loaded:** {total_dfs}")
            else:
                st.warning("No datasets found in the 'data' folder.")
    return crisis_years


def plot_correlation_matrix(df: pd.DataFrame, title: str):
    # æ’é™¤éæ•°å€¼åˆ—
    numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    correlation_matrix = df[numeric_columns].corr()
    # åˆ›å»ºå›¾å½¢
    plt.figure(figsize=(15, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True, linewidths=0.5 )
    plt.title(title)


def lasso_feature_selection(df, target_column='GDP', test_size=0.2, random_state=42):
    """
    Performs LASSO regression for feature selection on a dataset.

    Parameters:
        df (pd.DataFrame): The dataset containing economic indicators and a target variable.
        target_column (str): The name of the column to predict (e.g., 'GDP').
        test_size (float): The proportion of the dataset to use for testing.
        random_state (int): Random seed for reproducibility.

    Returns:
        list: Selected features from LASSO.
    """
    # Drop target and non-numeric columns
    X = df.drop(columns=[target_column, 'Date'], errors='ignore')
    y = df[target_column]

    # Handle missing values by filling with the median
    X.fillna(X.median(), inplace=True)

    # Standardize the features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)

    # Define hyperparameter range for alpha
    params = {"alpha": np.linspace(0.00001, 10, 500)}

    # Use K-Fold cross-validation
    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)

    # Initialize LASSO and perform GridSearchCV
    lasso = Lasso()
    lasso_cv = GridSearchCV(lasso, param_grid=params, cv=kf)
    lasso_cv.fit(X_scaled, y)

    # Get best alpha parameter
    best_alpha = lasso_cv.best_params_["alpha"]
    print(f"Best Alpha: {best_alpha}")

    # Train LASSO with best alpha
    lasso = Lasso(alpha=best_alpha)
    lasso.fit(X_train, y_train)

    # Select important features (non-zero coefficients)
    selected_features = X.columns[lasso.coef_ != 0].tolist()
    print("Selected Features by LASSO:", selected_features)

    # Plot feature importance
    plt.figure(figsize=(15, 8))
    plt.bar(selected_features, lasso.coef_[lasso.coef_ != 0], color='blue')
    plt.xlabel("Features")
    plt.ylabel("LASSO Coefficient")
    plt.title("Feature Importance from LASSO Regression")
    plt.xticks(rotation=45)
    plt.show()

    return selected_features

def generate_chart_summary(fig, prompt):
    genai.configure(api_key="AIzaSyCOdwdzuUplcSuVFa-mgEGRDsljGwEaYZk")
    model = genai.GenerativeModel('gemini-2.0-flash-lite')
    buf = io.BytesIO()
    fig.savefig(buf, format='png')
    buf.seek(0)
    image_bytes = buf.read()
    base64_image = base64.b64encode(image_bytes).decode('utf-8')

    contents = [
        prompt,
        {"mime_type": "image/png", "data": base64_image}
    ]
    response = model.generate_content(contents)
    return response.text

def current():
    all_dfs = load_dataset("data", [2008, 2022])
    target_step_column = 'Cap rate'
    annotate_events = {
        "Russia Invades Ukraine": ("2022-02-24", True),
        "Price Cap not implemented \n(as above Energy Price Guarantee)": ("2023-03-01", False)
    }
    highlight_periods = [
        (
        pd.to_datetime('2022-10-01'), pd.to_datetime('2023-04-01'), 2500, "Energy Price Guarantee (Â£2,500)", 'crimson'),
        (pd.to_datetime('2023-04-01'), pd.to_datetime('2023-07-01'), 3000, "Energy Price Guarantee (Â£3,000)", 'purple')
    ]

    year_of_interest = 2022
    if year_of_interest in all_dfs:
        for file_name, df in all_dfs[year_of_interest]:  # æ­£ç¡®è§£åŒ…å…ƒç»„
            if target_step_column in df.columns:
                fig = plotting(df, file_name=file_name, step_column=target_step_column,
                               annotation=annotate_events, highlight=highlight_periods)
                st.pyplot(fig)
                plt.close(fig)  # å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾å†…å­˜

                # è¡¥å……æ–‡å­—æè¿°
                prompt = "Describe the chart in 1 paragraph, starting with describing the trend (include statistics if applicable), followed by the analysis in relation to economics knowledge. Your response should only include the answer. Do not provide any further explanation."
                summary = generate_chart_summary(fig, prompt=prompt)
                print("ä¸‹é¢æ˜¯æ‰“å°çš„å›¾åƒæè¿°")
                print("Summary of the chart:")
                print(summary)
                st.write("Summary of the chart:")  # åœ¨ç½‘é¡µä¸Šæ˜¾ç¤ºæ–‡å­—
                st.write(summary)  # åœ¨ç½‘é¡µä¸Šæ˜¾ç¤ºæ–‡å­—
                # st.write("(Generated by Gemini, does not constitute a suggestion, for reference only!)")  # åœ¨ç½‘é¡µä¸Šæ˜¾ç¤ºæ–‡å­—
                st.markdown(
                    "<p style='font-size:12px; color:gray;'>(Generated by Gemini, does not constitute a suggestion, for reference only!)</p>",
                    unsafe_allow_html=True
                )
    else:
        st.warning(f"No data found for year {year_of_interest}")

def merged_analysis():
    st.title('Financial Crisis Analysis Dashboard')

    # è·å–å¯ç”¨å¹´ä»½
    crisis_years = sorted(st.session_state.get("crisis_years", [2008, 2022]))

    # ç»Ÿä¸€æ§åˆ¶å‚æ•°
    selected_year = st.selectbox('Select analysis year', options=crisis_years, index=0, key='year_selector')

    # å›¾è¡¨æ˜¾ç¤ºé€‰é¡¹
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        show_individual = st.checkbox("Individual indicators", value=True)
    with col2:
        show_combined = st.checkbox("Combined indicators", value=True)
    with col3:
        show_heatmap = st.checkbox("Correlation heatmap", value=True)
    with col4:
        show_lasso = st.checkbox("LASSO feature selection", value=True)
    with col5:
        show_timeseries = st.checkbox("TimeSeries model", value=True)

    if st.button('Generate All Visualizations'):
        with st.spinner(f"Generating visualizations for {selected_year}..."):
            # ç¡®ä¿æ•°æ®å·²åŠ è½½
            if 'all_dfs_plot' not in st.session_state:
                st.session_state['all_dfs_plot'] = load_dataset("./data", crisis_years)

            all_dfs = st.session_state['all_dfs_plot']

            if selected_year not in all_dfs or not all_dfs[selected_year]:
                st.warning(f"No data available for {selected_year}")
                return

            # åˆå¹¶æ•°æ®
            merged_df = merge_dataframes_on_date(*[df for _, df in all_dfs[selected_year]])
            merged_df = normalisation(merged_df)

            # 1. æ˜¾ç¤ºåŸå§‹æ•°æ®é›†å›¾è¡¨
            if show_individual:
                st.header("Individual Indicators")
                for file_name, df in all_dfs[selected_year]:
                    with st.expander(f"Dataset: {file_name}", expanded=True):
                        plotting(df, file_name)
                        st.pyplot(plt)
                        plt.clf()
                if selected_year ==2022: current()

            # 2. æ˜¾ç¤ºåˆå¹¶æŒ‡æ ‡æŠ˜çº¿å›¾
            if show_combined:
                st.header("Combined Indicators")
                plotting(merged_df, f"Merged Indicators - {selected_year}")
                st.pyplot(plt)
                plt.clf()

            # 3. æ˜¾ç¤ºç›¸å…³ç³»æ•°çƒ­åŠ›å›¾
            if show_heatmap:
                st.header("Correlation Matrix")
                plot_correlation_matrix(merged_df, f"Correlation - {selected_year}")
                st.pyplot(plt)
                plt.clf()

            # 4. LASSOç‰¹å¾é€‰æ‹©
            if show_lasso:
                st.header("Lasso Regression")
                lasso_feature_selection(merged_df, "GDP")
                st.pyplot(plt)
                plt.clf()

            if show_timeseries:
                st.header("Time Series Regression")
                ts_model = TimeSeriesRegression(merged_df, lasso_feature_selection(merged_df, "GDP"), 'GDP', max_lag=6)
                ts_model.run()

def main():
    upload()
    select_data()
    merged_analysis()

if __name__ == "__main__":
    main()